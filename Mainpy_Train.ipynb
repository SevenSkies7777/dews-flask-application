{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#LSTM MOdel Precipitation\n",
    "import numpy as np\n",
    "#import mysql.connector\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sqlalchemy import create_engine, text\n",
    "from Milk_Production_Forecast_Model import run_precip_forecast_pipeline\n",
    "from Milk_Production_Forecast_Model import MilkProductionForecaster\n",
    "\n",
    "# Function to convert NumPy types to Python native types\n",
    "def convert_numpy_types(df):\n",
    "    # Create a copy to avoid modifying the original\n",
    "    result = df.copy()\n",
    "    \n",
    "    # Convert all columns with numpy dtypes to Python native types\n",
    "    for col in result.columns:\n",
    "        if pd.api.types.is_integer_dtype(result[col]):\n",
    "            result[col] = result[col].astype(int)\n",
    "        elif pd.api.types.is_float_dtype(result[col]):\n",
    "            result[col] = result[col].astype(float)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(\n",
    "    'mysql+mysqlconnector://root:Romans17:48@127.0.0.1/livelihoodzones_1'\n",
    ")\n",
    "#hha_outliers = HHA_Outliers()\n",
    "\n",
    "'''conn = mysql.connector.connect(\n",
    "        host='127.0.0.1',\n",
    "        user='root',\n",
    "        password='Romans17:48',\n",
    "        database='livelihoodzones'\n",
    "    )\n",
    "\n",
    "cursor = conn.cursor()\n",
    "'''\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT hh_livestock_milk_production_per_species.HhaQuestionnaireSessionId as qid, hha_questionnaire_sessions.CountyId, hha_questionnaire_sessions.LivelihoodZoneId,hha_questionnaire_sessions.WardId,hha_questionnaire_sessions.HouseHoldId, hha_questionnaire_sessions.SubCountyId,\n",
    "            data_collection_exercise.ExerciseStartDate, Sum(hh_livestock_milk_production_per_species.DailyQntyMilkedInLtrs) as amountmilked,Sum(hh_livestock_milk_production_per_species.DailyQntyConsumedInLtrs) as amountconsumed,Sum(hh_livestock_milk_production_per_species.DailyQntySoldInLtrs) as amountsold, Sum(hh_livestock_milk_production_per_species.PricePerLtr) as PricePerLtr,wards.Shapefile_wardName\n",
    "    FROM (hh_livestock_milk_production_per_species\n",
    "          LEFT JOIN hha_questionnaire_sessions ON (hh_livestock_milk_production_per_species.HhaQuestionnaireSessionId = hha_questionnaire_sessions.HhaQuestionnaireSessionId))\n",
    "          LEFT JOIN data_collection_exercise ON (hha_questionnaire_sessions.DataCollectionExerciseId = data_collection_exercise.DataCollectionExerciseId)LEFT JOIN wards ON (hha_questionnaire_sessions.WardId = wards.WardId)\n",
    "    WHERE (hha_questionnaire_sessions.CountyId = '46' )\n",
    "    GROUP BY hh_livestock_milk_production_per_species.HhaQuestionnaireSessionId, hha_questionnaire_sessions.CountyId, hha_questionnaire_sessions.LivelihoodZoneId,hha_questionnaire_sessions.WardId,hha_questionnaire_sessions.HouseHoldId, hha_questionnaire_sessions.SubCountyId,data_collection_exercise.ExerciseStartDate, wards.Shapefile_wardName\n",
    "\"\"\"\n",
    "\n",
    "#db_df1 = pd.read_sql(query, conn)\n",
    "db_df1 = pd.read_sql(query, engine)\n",
    "\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT Seasons.season,Seasons.Season_Index, Seasons.Month,LTAs.Bad_year, LTAs.Good_year\n",
    "    FROM Seasons LEFT JOIN LTAs ON (Seasons.month = LTAs.month)\n",
    "   WHERE (LTAs.CountyId = '46')\n",
    "    \"\"\"\n",
    "\n",
    "#Seasons = pd.read_sql(query, conn)\n",
    "Seasons = pd.read_sql(query, engine)\n",
    "\n",
    "db_df1['year'] = db_df1['ExerciseStartDate'].dt.year\n",
    "db_df1['month'] = db_df1['ExerciseStartDate'].dt.strftime('%B') \n",
    "db_df1['month_num'] = db_df1['ExerciseStartDate'].dt.month\n",
    "\n",
    "db_df = db_df1.merge(Seasons, left_on=['month'], right_on=['Month'], how='right')\n",
    "\n",
    "#conn.close()\n",
    "#db_df\n",
    "\n",
    "'''conn = mysql.connector.connect(\n",
    "        host='127.0.0.1',\n",
    "        user='root',\n",
    "        password='Romans17:48',\n",
    "        database='livelihoodzones'\n",
    "    )\n",
    "\n",
    "cursor = conn.cursor()\n",
    "'''\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM Precipitation LEFT JOIN counties ON (counties.CountyName = Precipitation.NAME_1)\n",
    "    WHERE (counties.CountyId = '46')\n",
    "    \"\"\"\n",
    "\n",
    "#precipitation_df = pd.read_sql(query, conn)\n",
    "precipitation_df = pd.read_sql(query, engine)\n",
    "\n",
    "\n",
    "prep_df0 = precipitation_df.groupby(['NAME_3','T'])['precipitation'].sum()\n",
    "prep_df0 = prep_df0.reset_index()\n",
    "\n",
    "#conn.close()\n",
    "prep_df0['T'] = pd.to_datetime(prep_df0['T'], errors='coerce')\n",
    "prep_df0['year'] = prep_df0['T'].dt.year\n",
    "prep_df0['month_name'] = prep_df0['T'].dt.strftime('%B') \n",
    "prep_df0['month_num'] = prep_df0['T'].dt.month\n",
    "prep_df0\n",
    "prep_df0 = Seasons.merge(prep_df0, left_on=['Month'], right_on=['month_name'], how='right')\n",
    "#prep_df0\n",
    "#Wusi/Kishamba Chala\n",
    "#unique_wards = prep_df0[\"NAME_3\"].unique()\n",
    "unique_wards = db_df1[db_df1['Shapefile_wardName'].notna()]\n",
    "unique_wards = unique_wards['Shapefile_wardName'].unique()\n",
    "#unique_wards = ['Chala']\n",
    "for NAME_3 in unique_wards:\n",
    "    print(f\"Processing {NAME_3}...\")\n",
    "    prep_df = prep_df0[prep_df0[\"NAME_3\"] == NAME_3]\n",
    "    prep_df = prep_df.reset_index()    \n",
    "    prep_df=prep_df[['season','Season_Index','Month','NAME_3','T','precipitation','year','month_name','month_num']]\n",
    "    unique_ward = prep_df[\"NAME_3\"].unique()\n",
    "    #prep_df1=prep_df\n",
    "    #prep_df['T'] = pd.to_datetime(prep_df['T'])\n",
    "    #pd.to_datetime(prep_df0['T'], errors='coerce')\n",
    "    #cutoff_date = pd.to_datetime('2024-0-01')\n",
    "    #prep_df=prep_df[(prep_df['T'] < cutoff_date)]\n",
    "\n",
    "    # Call the precipitation forecasting function\n",
    "    results = run_precip_forecast_pipeline(\n",
    "        prep_df=prep_df,\n",
    "        seq_length=48,\n",
    "        forecast_start_year=2016,\n",
    "        forecast_start_month=1,\n",
    "        n_future=109\n",
    "    )\n",
    "\n",
    "    # Access the results\n",
    "    model = results[\"model\"]\n",
    "    scaler = results[\"scaler\"]\n",
    "    forecast_df = results[\"forecast_df\"]\n",
    "    test_metrics = results[\"test_metrics\"]\n",
    "\n",
    "    print(f\"RMSE: {test_metrics.get('rmse')}\")\n",
    "    print(f\"Forecast periods with actuals: {results['forecast_metrics'].get('n_with_actuals', 0)}\")\n",
    "\n",
    "    if not forecast_df.empty:\n",
    "        print(f\"Forecast first 5 periods: \\n{forecast_df[['Date', 'Forecasted Precipitation']].head()}\")\n",
    "\n",
    "    print(forecast_df.head())\n",
    "\n",
    "    forecast_df1=forecast_df[['Month','Year','Date_Object','Forecasted Precipitation','Forecast Uncertainty (Std Dev)']]\n",
    "    prep_df1 = forecast_df1.merge(prep_df, left_on=['Month','Year','Date_Object'], right_on=['month_num','year','T'], how='right')\n",
    "    prep_df2=prep_df1[['year','Forecasted Precipitation','NAME_3','T','precipitation','month_name','month_num']]\n",
    "    precipitation_forecasts_df=prep_df2\n",
    "    db_df=db_df[['WardId','HouseHoldId','Shapefile_wardName', 'month', 'year', 'season','Season_Index','amountmilked','Bad_year','Good_year']]\n",
    "\n",
    "\n",
    "    #Cleaning data (Outliers)\n",
    "    def replace_outliers_with_averages(db_df, variables):\n",
    "        \"\"\"\n",
    "        Replace outliers with the average value for each ward and month combination.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        db_df : pandas.DataFrame\n",
    "            DataFrame with outlier detection results\n",
    "        variables : list\n",
    "            List of variable names to process\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            DataFrame with outliers replaced by averages\n",
    "        \"\"\"\n",
    "        # Create a clean version with outliers replaced by averages\n",
    "        db_df_clean = db_df.copy()\n",
    "\n",
    "        if 'year_month' not in db_df.columns:\n",
    "            db_df_clean['year_month'] = db_df_clean['year'].astype(str) + '-' + db_df_clean['month'].astype(str)\n",
    "        \n",
    "        for variable in variables:\n",
    "            print(f\"\\nReplacing outliers for {variable} with ward-month averages\")\n",
    "            outlier_col = f'{variable}_is_outlier'\n",
    "            \n",
    "            # Check if outlier column exists\n",
    "            if outlier_col not in db_df.columns:\n",
    "                print(f\"Warning: No outlier data found for {variable}, skipping replacement\")\n",
    "                continue\n",
    "\n",
    "            for ward in db_df_clean['ward'].unique():\n",
    "                for year_month in db_df_clean['year_month'].unique():\n",
    "\n",
    "                    mask = (db_df_clean['ward'] == ward) & (db_df_clean['year_month'] == year_month)\n",
    "                    subset = db_df_clean[mask]\n",
    "                    \n",
    "                    # Skip if no data\n",
    "                    if len(subset) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    # Skip if insufficient data\n",
    "                    if len(subset) <= 2:\n",
    "                        print(f\"Warning: Ward {ward} in {year_month} has insufficient data for average calculation\")\n",
    "                        continue\n",
    "                        \n",
    "                    # Check if there are any outliers in this group\n",
    "                    if not subset[outlier_col].any():\n",
    "                        continue\n",
    "                    \n",
    "                    # Check if there are any non-outliers to calculate average from\n",
    "                    if subset[outlier_col].all():\n",
    "                        print(f\"Warning: All values in ward {ward}, {year_month} for {variable} are outliers\")\n",
    "\n",
    "                        ward_avg = db_df_clean.loc[(db_df_clean['ward'] == ward) & ~db_df_clean[outlier_col], variable].mean()\n",
    "                        if np.isnan(ward_avg):\n",
    "                            print(f\"Cannot find suitable replacement for ward {ward}, {year_month}. Keeping original values.\")\n",
    "                            continue\n",
    "                        replacement_value = ward_avg\n",
    "                    else:\n",
    "\n",
    "                        non_outlier_mean = subset.loc[~subset[outlier_col], variable].mean()\n",
    "                        replacement_value = non_outlier_mean\n",
    "\n",
    "                    outlier_mask = mask & db_df_clean[outlier_col]\n",
    "                    if outlier_mask.any():\n",
    "                        db_df_clean.loc[outlier_mask, variable] = replacement_value\n",
    "                        print(f\"Replaced {outlier_mask.sum()} outliers in ward {ward}, {year_month} for {variable}\")\n",
    "        \n",
    "        return db_df_clean\n",
    "\n",
    "    db_df_clean = replace_outliers_with_averages(db_df, variables=['amountmilked'])\n",
    "    db_df_clean\n",
    "\n",
    "    unique_ward\n",
    "    unique_ward1 = unique_ward[0]\n",
    "    unique_ward2 = unique_ward[0]\n",
    "    unique_ward1\n",
    "\n",
    "    db_df_clean1=db_df_clean.groupby(['Shapefile_wardName', 'month', 'year', 'season','Season_Index','Bad_year','Good_year'])[['amountmilked']].mean().reset_index()\n",
    "\n",
    "    joined_data2 = db_df_clean1.merge(prep_df2, left_on=['Shapefile_wardName', 'year', 'month'], right_on=['NAME_3', 'year', 'month_name'], how='right')\n",
    "\n",
    "    joined_data3=joined_data2[(joined_data2['Shapefile_wardName']==unique_ward1)&(joined_data2['year']>2016)]\n",
    "\n",
    "    data_numeric = joined_data3.assign(**{col: joined_data3[col].map(lambda x: x.toordinal()) \n",
    "                                        for col in joined_data3.select_dtypes(include=['datetime64'])})\n",
    "    \n",
    "\n",
    "    data_numeric = data_numeric.sort_values(by=\"T\")\n",
    "    print(data_numeric.head())\n",
    "\n",
    "    features = [\"year\", \"month_num\", \"Season_Index\", \"precipitation\", \n",
    "                \"Forecasted Precipitation\", \"amountmilked\", \"months_gap\"]\n",
    "\n",
    "\n",
    "    # Call the function Milk_forecasting funtion\n",
    "    if data_numeric is None or data_numeric.empty:\n",
    "        print(\"Warning: data_numeric is empty or None. Cannot proceed with forecasting.\")\n",
    "\n",
    "        results = {\n",
    "            'model': None,\n",
    "            'scaler': None,\n",
    "            'training_history': None,\n",
    "            'evaluation_metrics': {},\n",
    "            'test_results': pd.DataFrame(),\n",
    "            'forecast_results': pd.DataFrame(),\n",
    "            'feature_indices': {},\n",
    "            'data_month_to_season': {}\n",
    "        }\n",
    "        forecast_df = results[\"forecast_results\"]\n",
    "        test_results = results[\"test_results\"]\n",
    "        evaluation_metrics = results[\"evaluation_metrics\"]\n",
    "\n",
    "    else:\n",
    "\n",
    "        results = MilkProductionForecaster(\n",
    "            data_numeric.copy(),\n",
    "            features=features,\n",
    "            n_future=16,\n",
    "            external_precip_forecasts=precipitation_forecasts_df\n",
    "        )\n",
    "        # Access the results\n",
    "        \n",
    "        forecast_df = results[\"forecast_results\"]\n",
    "        test_results = results[\"test_results\"]\n",
    "        evaluation_metrics = results[\"evaluation_metrics\"]\n",
    "\n",
    "        print(results['forecast_results'][['Ward', 'Month', 'Year', 'Forecasted Amount Milked']].head())\n",
    "        forecast_df.rename(columns={'Months Gap': 'Months_Gap','Forecasted Amount Milked': 'Forecasted_Value','Actual (if available)': 'Actual','Forecast Uncertainty (Std Dev)': 'Forecast_Uncertainty','Lower Bound (95%)': 'Lower_Bound','Upper Bound (95%)': 'Upper_Bound','Percent Error': 'Percent_Error'}, inplace=True)\n",
    "        forecast_df5=forecast_df[['Month','Year','Season_Index','Precipitation','Months_Gap','Date','Date_Object','Forecasted_Value','Actual','Forecast_Uncertainty','Lower_Bound','Upper_Bound','Error','Percent_Error','Last_Actual_Value','Month1_Forecast','Month2_Forecast','Month3_Forecast','Evaluation_Metrics']]\n",
    "        forecast_df5['Ward']=unique_ward2\n",
    "        forecast_df5['Indicator']=\"TotalDailyQntyMilkedInLtrs\"\n",
    "        forecast_df5=forecast_df5[['Ward','Month','Year','Season_Index','Precipitation','Months_Gap','Date','Date_Object','Indicator','Forecasted_Value','Actual','Forecast_Uncertainty','Lower_Bound','Upper_Bound','Error','Percent_Error','Last_Actual_Value','Month1_Forecast','Month2_Forecast','Month3_Forecast','Evaluation_Metrics']]\n",
    "        forecast_df5\n",
    "\n",
    "        # Inserting results into DB\n",
    "        for col in forecast_df5.columns:\n",
    "\n",
    "            if pd.api.types.is_numeric_dtype(forecast_df5[col]):\n",
    "                forecast_df5[col] = forecast_df5[col].apply(\n",
    "                    lambda x: float(x) if not pd.isna(x) else None\n",
    "                )\n",
    "\n",
    "            elif pd.api.types.is_datetime64_dtype(forecast_df5[col]):\n",
    "                forecast_df5[col] = forecast_df5[col].apply(\n",
    "                    lambda x: x.strftime('%Y-%m-%d %H:%M:%S') if not pd.isna(x) else None\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                forecast_df5[col] = forecast_df5[col].apply(\n",
    "                    lambda x: str(x) if not pd.isna(x) else None\n",
    "                )\n",
    "\n",
    "        if 'Date_Object' in forecast_df5.columns:\n",
    "            forecast_df5['Date_Object'] = forecast_df5['Date_Object'].apply(\n",
    "                lambda x: x.strftime('%Y-%m-%d %H:%M:%S') if hasattr(x, 'strftime') and x is not None \n",
    "                        else x if isinstance(x, str) else None\n",
    "            )\n",
    "\n",
    "        existing_special_rows = []\n",
    "        try:\n",
    "            query = text(\"\"\"\n",
    "                SELECT Ward, Month, Year FROM Predictions \n",
    "                WHERE Last_Actual_Value IS NOT NULL\n",
    "            \"\"\")\n",
    "            with engine.connect() as conn:\n",
    "                result = conn.execute(query)\n",
    "                existing_special_rows = [(row[0], row[1], row[2]) for row in result]\n",
    "            print(f\"Found {len(existing_special_rows)} existing special rows in database\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking for existing special rows: {e}\")\n",
    "\n",
    "        special_rows_df = forecast_df5[forecast_df5['Last_Actual_Value'].notnull()].copy()\n",
    "        special_rows_keys = [(row['Ward'], row['Month'], row['Year']) for _, row in special_rows_df.iterrows()]\n",
    "        print(f\"Found {len(special_rows_keys)} special rows in forecast_df5\")\n",
    "\n",
    "        insert_count = 0\n",
    "        skip_count = 0\n",
    "\n",
    "        for idx, row in forecast_df5.iterrows():\n",
    "            ward = row['Ward']\n",
    "            month = row['Month']\n",
    "            year = row['Year']\n",
    "            row_key = (ward, month, year)\n",
    "            is_special = row['Last_Actual_Value'] is not None\n",
    "\n",
    "            if is_special and row_key not in existing_special_rows:\n",
    "                try:\n",
    "                    delete_query = text(\"\"\"\n",
    "                        DELETE FROM Predictions \n",
    "                        WHERE Ward = :ward AND Month = :month AND Year = :year\n",
    "                    \"\"\")\n",
    "                    with engine.begin() as conn:\n",
    "                        conn.execute(delete_query, {\"ward\": ward, \"month\": month, \"year\": year})\n",
    "                    print(f\"Cleared existing rows for new special row {row_key}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error clearing rows for new special row {row_key}: {e}\")\n",
    "\n",
    "                should_insert = True\n",
    "\n",
    "            elif not is_special and row_key not in existing_special_rows:\n",
    "\n",
    "                try:\n",
    "                    delete_query = text(\"\"\"\n",
    "                        DELETE FROM Predictions \n",
    "                        WHERE Ward = :ward AND Month = :month AND Year = :year\n",
    "                    \"\"\")\n",
    "                    with engine.begin() as conn:\n",
    "                        conn.execute(delete_query, {\"ward\": ward, \"month\": month, \"year\": year})\n",
    "                except Exception as e:\n",
    "                    print(f\"Error clearing rows for non-special row {row_key}: {e}\")\n",
    "\n",
    "                should_insert = True\n",
    "            \n",
    "            elif is_special and row_key in existing_special_rows:\n",
    "\n",
    "                try:\n",
    "                    delete_query = text(\"\"\"\n",
    "                        DELETE FROM Predictions \n",
    "                        WHERE Ward = :ward AND Month = :month AND Year = :year\n",
    "                    \"\"\")\n",
    "                    with engine.begin() as conn:\n",
    "                        conn.execute(delete_query, {\"ward\": ward, \"month\": month, \"year\": year})\n",
    "                    print(f\"Deleted existing special row {row_key} for update\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error deleting existing special row {row_key} for update: {e}\")\n",
    "\n",
    "                should_insert = True\n",
    "\n",
    "            else:  \n",
    "                print(f\"Skipping non-special row {row_key} because special row exists in DB\")\n",
    "                skip_count += 1\n",
    "                should_insert = False\n",
    "\n",
    "            if should_insert:\n",
    "                params = {}\n",
    "                for col in forecast_df5.columns:\n",
    "                    value = row[col]\n",
    "\n",
    "                    if hasattr(value, 'dtype'):\n",
    "                        if np.issubdtype(value.dtype, np.integer):\n",
    "                            params[col.lower()] = int(value)\n",
    "                        elif np.issubdtype(value.dtype, np.floating):\n",
    "                            params[col.lower()] = float(value) if not np.isnan(value) else None\n",
    "                        else:\n",
    "                            params[col.lower()] = str(value) if value is not None else None\n",
    "                    elif pd.isna(value):\n",
    "                        params[col.lower()] = None\n",
    "                    else:\n",
    "                        params[col.lower()] = value\n",
    "\n",
    "                if 'indicator' not in params:\n",
    "                    params['indicator'] = 'amountmilked'  \n",
    "                                  \n",
    "                if 'forecasted_value' not in params and 'forecasted_amount_milked' in params:\n",
    "                    params['forecasted_value'] = params['forecasted_amount_milked']\n",
    "                                \n",
    "                try:\n",
    "                    inspect_query = text(\"\"\"\n",
    "                        SHOW COLUMNS FROM Predictions\n",
    "                    \"\"\")\n",
    "                    with engine.connect() as conn:\n",
    "                        result = conn.execute(inspect_query)\n",
    "                        db_columns = [row[0].lower() for row in result]\n",
    "                        \n",
    "                    \n",
    "                    columns = []\n",
    "                    values = []\n",
    "                    insert_params = {}\n",
    "                    \n",
    "                    for key, value in params.items():\n",
    "                        if key.lower() in db_columns:\n",
    "                            columns.append(key.lower())\n",
    "                            values.append(f\":{key.lower()}\")\n",
    "                            insert_params[key.lower()] = value\n",
    "                    \n",
    "                    \n",
    "                    columns_str = \", \".join(columns)\n",
    "                    values_str = \", \".join(values)\n",
    "                    insert_query = text(f\"\"\"\n",
    "                        INSERT INTO Predictions ({columns_str}) \n",
    "                        VALUES ({values_str})\n",
    "                    \"\"\")\n",
    "                    \n",
    "                    with engine.begin() as conn:\n",
    "                        conn.execute(insert_query, insert_params)\n",
    "                    \n",
    "                    insert_count += 1\n",
    "                    if is_special:\n",
    "                        print(f\"Inserted/updated special row {row_key}\")\n",
    "                    elif insert_count % 10 == 0:\n",
    "                        print(f\"Inserted {insert_count} rows so far\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error inserting record {row_key}: {e}\")\n",
    "                    print(\"Parameters used:\")\n",
    "                    for key, value in params.items():\n",
    "                        print(f\"  {key}: {value} (type: {type(value)})\")\n",
    "\n",
    "        print(f\"Inserted/updated {insert_count} rows, skipped {skip_count} rows\")\n",
    "        print(\"Process completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
